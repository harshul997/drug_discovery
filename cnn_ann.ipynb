{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50443b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score, precision_recall_curve, auc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def seed_everything(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "seed_everything()\n",
    "\n",
    "# Define device (use MPS if available, otherwise use CPU)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf2a5c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        drug_id       prot_id  \\\n",
      "0      11314340          AAK1   \n",
      "1      11314340   ABL1(E255K)   \n",
      "2      11314340   ABL1(F317I)   \n",
      "3      11314340  ABL1(F317I)p   \n",
      "4      11314340   ABL1(F317L)   \n",
      "...         ...           ...   \n",
      "30051    151194           YES   \n",
      "30052    151194          YSK1   \n",
      "30053    151194          YSK4   \n",
      "30054    151194           ZAK   \n",
      "30055    151194         ZAP70   \n",
      "\n",
      "                                                  SMILES  \\\n",
      "0      CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...   \n",
      "1      CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...   \n",
      "2      CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...   \n",
      "3      CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...   \n",
      "4      CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...   \n",
      "...                                                  ...   \n",
      "30051  C1=CC=C2C(=C1)C(=NN=C2NC3=CC=C(C=C3)Cl)CC4=CC=...   \n",
      "30052  C1=CC=C2C(=C1)C(=NN=C2NC3=CC=C(C=C3)Cl)CC4=CC=...   \n",
      "30053  C1=CC=C2C(=C1)C(=NN=C2NC3=CC=C(C=C3)Cl)CC4=CC=...   \n",
      "30054  C1=CC=C2C(=C1)C(=NN=C2NC3=CC=C(C=C3)Cl)CC4=CC=...   \n",
      "30055  C1=CC=C2C(=C1)C(=NN=C2NC3=CC=C(C=C3)Cl)CC4=CC=...   \n",
      "\n",
      "                                             sequence_aa  \\\n",
      "0      MKKFFDSRREQGGSGLGSGSSGGGGSTSGLGSGYIGRVFGIGRQQV...   \n",
      "1      PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...   \n",
      "2      PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...   \n",
      "3      PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...   \n",
      "4      PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...   \n",
      "...                                                  ...   \n",
      "30051  MGCIKSKENKSPAIKYRPENTPEPVSTSVSHYGAEPTTVSPCPSSS...   \n",
      "30052  MAHLRGFANQHSRVDPEELFTKLDRIGKGSFGEVYKGIDNHTKEVV...   \n",
      "30053  MSSMPKPERHAESLLDICHDTNSSPTDLMTVTKNQNIILQSISRSE...   \n",
      "30054  MSSLGASFVQIKFDDLQFFENCGGGSFGSVYRAKWISQDKEVAVKK...   \n",
      "30055  MPDPAAHLPFFYGSISRAEAEEHLKLAGMADGLFLLRQCLRSLGGY...   \n",
      "\n",
      "       sequence_aa_affinity                                   encoded_sequence  \n",
      "0                  7.366532  [11, 9, 9, 5, 5, 3, 16, 15, 15, 4, 14, 6, 6, 1...  \n",
      "1                  5.000000  [13, 5, 19, 9, 8, 10, 12, 13, 10, 10, 4, 15, 6...  \n",
      "2                  5.000000  [13, 5, 19, 9, 8, 10, 12, 13, 10, 10, 4, 15, 6...  \n",
      "3                  5.000000  [13, 5, 19, 9, 8, 10, 12, 13, 10, 10, 4, 15, 6...  \n",
      "4                  5.000000  [13, 5, 19, 9, 8, 10, 12, 13, 10, 10, 4, 15, 6...  \n",
      "...                     ...                                                ...  \n",
      "30051              5.000000  [11, 6, 2, 8, 9, 16, 9, 4, 12, 9, 16, 13, 1, 8...  \n",
      "30052              5.000000  [11, 1, 7, 10, 15, 6, 5, 1, 12, 14, 7, 16, 15,...  \n",
      "30053              5.721246  [11, 16, 16, 11, 13, 9, 13, 4, 15, 7, 1, 4, 16...  \n",
      "30054              5.356547  [11, 16, 16, 10, 6, 1, 16, 5, 18, 14, 8, 9, 5,...  \n",
      "30055              5.000000  [11, 13, 3, 13, 1, 1, 7, 10, 13, 5, 5, 21, 6, ...  \n",
      "\n",
      "[30056 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "data_path = 'davis.txt'  # Update with correct path\n",
    "columns = ['drug_id', 'prot_id', 'SMILES', 'sequence_aa', 'sequence_aa_affinity']\n",
    "df = pd.read_csv(data_path, sep=' ', names=columns)\n",
    "# print(df)\n",
    "# Encode 20 different amino acids with unique numbers\n",
    "def encode_amino_acids(sequence):\n",
    "    unique_aa = sorted(set(''.join(sequence)))\n",
    "    aa_to_int = {aa: i+1 for i, aa in enumerate(unique_aa)}  # Map each AA to a unique number\n",
    "    return [[aa_to_int[aa] for aa in seq] for seq in sequence]\n",
    "\n",
    "df['encoded_sequence'] = encode_amino_acids(df['sequence_aa'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6462b2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import MolFromSmiles\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "def encode_smiles(smiles_list, n_bits=2):\n",
    "    fingerprints = []\n",
    "    for smi in smiles_list:\n",
    "        mol = MolFromSmiles(smi)\n",
    "        if mol:\n",
    "            fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=n_bits)\n",
    "            fingerprints.append(np.array(fp))\n",
    "        else:\n",
    "            fingerprints.append(np.zeros(n_bits))  # Default vector for invalid SMILES\n",
    "    return np.array(fingerprints)\n",
    "\n",
    "df['encoded_SMILES'] = list(encode_smiles(df['SMILES']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bda3555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def process_data(df):\n",
    "    # 1. Pad the amino acid sequences\n",
    "    def pad_sequences(sequences, max_len):\n",
    "        return [\n",
    "            seq + [0] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len]\n",
    "            for seq in sequences\n",
    "        ]\n",
    "    \n",
    "    # Get the max length of amino acid sequences for padding\n",
    "    max_aa_len = max(df['encoded_sequence'].apply(len))\n",
    "\n",
    "    # Pad the amino acid sequences\n",
    "    df['padded_sequence'] = pad_sequences(df['encoded_sequence'].tolist(), max_aa_len)\n",
    "\n",
    "    # Convert padded amino acid sequences to a PyTorch tensor\n",
    "    X_amino = torch.tensor(df['padded_sequence'].values.tolist(), dtype=torch.float32)\n",
    "\n",
    "    # 2. Preprocess and pad SMILES sequences\n",
    "    # Ensure no NaNs or infinities in encoded SMILES\n",
    "    df['encoded_SMILES'] = df['encoded_SMILES'].apply(lambda x: np.nan_to_num(x))\n",
    "\n",
    "    # Get the max length of SMILES sequences for padding\n",
    "    max_smiles_len = max([len(item) for item in df['encoded_SMILES']])\n",
    "\n",
    "    # Pad the SMILES sequences\n",
    "    padded_smiles = [np.pad(item, (0, max_smiles_len - len(item))) for item in df['encoded_SMILES']]\n",
    "\n",
    "    # Convert the padded SMILES sequences to a NumPy array\n",
    "    X_smiles_np = np.vstack(padded_smiles)\n",
    "\n",
    "    # 3. Process SMILES in smaller batches to prevent memory overload\n",
    "    batch_size = 1000  # Adjust the batch size depending on available memory\n",
    "    chunks = []\n",
    "\n",
    "    # Split the SMILES data into chunks and convert each chunk to a tensor\n",
    "    for i in range(0, X_smiles_np.shape[0], batch_size):\n",
    "        chunk = X_smiles_np[i:i+batch_size]\n",
    "        chunks.append(torch.tensor(chunk, dtype=torch.float32))  # Convert to float32\n",
    "\n",
    "    # Concatenate all chunks into one large tensor\n",
    "    X_smiles = torch.cat(chunks, dim=0)\n",
    "\n",
    "    return X_amino, X_smiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0abf75",
   "metadata": {},
   "source": [
    "## Now 2 cases of splitting, for first, test contains no new proteins, for 2nd one test contains new proteins\n",
    "\n",
    "## same for drugs, so 4 cases, 4 snippets for train test split,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f31c254b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protein Split 70/30: Train: 20774 Test: 9282\n",
      "Protein Split New Proteins: Train: 26996 Test: 3060\n",
      "Drug Split 70/30: Train: 21012 Test: 9044\n",
      "Drug Split New Drugs: Train: 26962 Test: 3094\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming df is the loaded dataset\n",
    "# Assuming labels are in a column named 'labels'\n",
    "\n",
    "# Case 1: Protein-based split (70/30 for each protein)\n",
    "def protein_split_70_30(df):\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "\n",
    "    for protein in df['prot_id'].unique():\n",
    "        protein_indices = df[df['prot_id'] == protein].index\n",
    "        train_idx, test_idx = train_test_split(protein_indices, test_size=0.3, random_state=42)\n",
    "        train_indices.extend(train_idx)\n",
    "        test_indices.extend(test_idx)\n",
    "\n",
    "    train_df = df.loc[train_indices]\n",
    "    test_df = df.loc[test_indices]\n",
    "    train_labels = train_df['sequence_aa_affinity']\n",
    "    test_labels = test_df['sequence_aa_affinity']\n",
    "    train_df = train_df.drop(columns=['sequence_aa_affinity'])\n",
    "    test_df = test_df.drop(columns=['sequence_aa_affinity'])\n",
    "    return train_df, test_df, train_labels, test_labels\n",
    "\n",
    "# Case 2: Protein-based split (90/10 with new proteins in test)\n",
    "def protein_split_new_proteins(df):\n",
    "    proteins = df['prot_id'].unique()\n",
    "    train_proteins, test_proteins = train_test_split(proteins, test_size=0.1, random_state=42)\n",
    "\n",
    "    train_df = df[df['prot_id'].isin(train_proteins)]\n",
    "    test_df = df[df['prot_id'].isin(test_proteins)]\n",
    "    train_labels = train_df['sequence_aa_affinity']\n",
    "    test_labels = test_df['sequence_aa_affinity']\n",
    "    train_df = train_df.drop(columns=['sequence_aa_affinity'])\n",
    "    test_df = test_df.drop(columns=['sequence_aa_affinity'])\n",
    "    return train_df, test_df, train_labels, test_labels\n",
    "\n",
    "# Case 3: Drug-based split (70/30 for each drug)\n",
    "def drug_split_70_30(df):\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "\n",
    "    for drug in df['drug_id'].unique():\n",
    "        drug_indices = df[df['drug_id'] == drug].index\n",
    "        train_idx, test_idx = train_test_split(drug_indices, test_size=0.3, random_state=42)\n",
    "        train_indices.extend(train_idx)\n",
    "        test_indices.extend(test_idx)\n",
    "\n",
    "    train_df = df.loc[train_indices]\n",
    "    test_df = df.loc[test_indices]\n",
    "    train_labels = train_df['sequence_aa_affinity']\n",
    "    test_labels = test_df['sequence_aa_affinity']\n",
    "    train_df = train_df.drop(columns=['sequence_aa_affinity'])\n",
    "    test_df = test_df.drop(columns=['sequence_aa_affinity'])\n",
    "    return train_df, test_df, train_labels, test_labels\n",
    "\n",
    "# Case 4: Drug-based split (90/10 with new drugs in test)\n",
    "def drug_split_new_drugs(df):\n",
    "    drugs = df['drug_id'].unique()\n",
    "    train_drugs, test_drugs = train_test_split(drugs, test_size=0.1, random_state=42)\n",
    "\n",
    "    train_df = df[df['drug_id'].isin(train_drugs)]\n",
    "    test_df = df[df['drug_id'].isin(test_drugs)]\n",
    "    train_labels = train_df['sequence_aa_affinity']\n",
    "    test_labels = test_df['sequence_aa_affinity']\n",
    "    train_df = train_df.drop(columns=['sequence_aa_affinity'])\n",
    "    test_df = test_df.drop(columns=['sequence_aa_affinity'])\n",
    "    return train_df, test_df, train_labels, test_labels\n",
    "\n",
    "# Perform splits\n",
    "train_df_1, test_df_1, train_labels_1, test_labels_1 = protein_split_70_30(df)\n",
    "train_df_2, test_df_2, train_labels_2, test_labels_2 = protein_split_new_proteins(df)\n",
    "train_df_3, test_df_3, train_labels_3, test_labels_3 = drug_split_70_30(df)\n",
    "train_df_4, test_df_4, train_labels_4, test_labels_4 = drug_split_new_drugs(df)\n",
    "\n",
    "# Display sizes\n",
    "print(\"Protein Split 70/30: Train:\", len(train_df_1), \"Test:\", len(test_df_1))\n",
    "print(\"Protein Split New Proteins: Train:\", len(train_df_2), \"Test:\", len(test_df_2))\n",
    "print(\"Drug Split 70/30: Train:\", len(train_df_3), \"Test:\", len(test_df_3))\n",
    "print(\"Drug Split New Drugs: Train:\", len(train_df_4), \"Test:\", len(test_df_4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eaf8ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for case 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dy/nh23qmtn6n3g6c50qc9w1fhm0000gn/T/ipykernel_24016/310495481.py:90: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:277.)\n",
      "  X_smiles_train = torch.tensor(train_df['encoded_SMILES'].tolist(), dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/400, Loss: 22.2174\n",
      "Epoch 20/400, Loss: 16.7940\n",
      "Epoch 30/400, Loss: 11.9762\n",
      "Epoch 40/400, Loss: 7.4259\n",
      "Epoch 50/400, Loss: 3.7405\n",
      "Epoch 60/400, Loss: 1.6152\n",
      "Epoch 70/400, Loss: 0.8071\n",
      "Epoch 80/400, Loss: 0.7271\n",
      "Epoch 90/400, Loss: 0.7403\n",
      "Epoch 100/400, Loss: 0.7088\n",
      "Epoch 110/400, Loss: 0.7072\n",
      "Epoch 120/400, Loss: 0.7013\n",
      "Epoch 130/400, Loss: 0.7005\n",
      "Epoch 140/400, Loss: 0.6999\n",
      "Epoch 150/400, Loss: 0.6997\n",
      "Epoch 160/400, Loss: 0.6997\n",
      "Epoch 170/400, Loss: 0.6996\n",
      "Epoch 180/400, Loss: 0.6996\n",
      "Epoch 190/400, Loss: 0.6996\n",
      "Epoch 200/400, Loss: 0.6996\n",
      "Epoch 210/400, Loss: 0.6996\n",
      "Epoch 220/400, Loss: 0.6996\n",
      "Epoch 230/400, Loss: 0.6996\n",
      "Epoch 240/400, Loss: 0.6996\n",
      "Epoch 250/400, Loss: 0.6996\n",
      "Epoch 260/400, Loss: 0.6996\n",
      "Epoch 270/400, Loss: 0.6996\n",
      "Epoch 280/400, Loss: 0.6996\n",
      "Epoch 290/400, Loss: 0.6996\n",
      "Epoch 300/400, Loss: 0.6996\n",
      "Epoch 310/400, Loss: 0.6996\n",
      "Epoch 320/400, Loss: 0.6996\n",
      "Epoch 330/400, Loss: 0.6996\n",
      "Epoch 340/400, Loss: 0.6996\n",
      "Epoch 350/400, Loss: 0.6996\n",
      "Epoch 360/400, Loss: 0.6996\n",
      "Epoch 370/400, Loss: 0.6996\n",
      "Epoch 380/400, Loss: 0.6996\n",
      "Epoch 390/400, Loss: 0.6996\n",
      "Epoch 400/400, Loss: 0.6996\n",
      "Training model for case 2...\n",
      "Epoch 10/400, Loss: 21.6383\n",
      "Epoch 20/400, Loss: 16.0159\n",
      "Epoch 30/400, Loss: 10.4502\n",
      "Epoch 40/400, Loss: 4.9866\n",
      "Epoch 50/400, Loss: 1.9726\n",
      "Epoch 60/400, Loss: 1.0105\n",
      "Epoch 70/400, Loss: 0.7548\n",
      "Epoch 80/400, Loss: 0.7510\n",
      "Epoch 90/400, Loss: 0.7437\n",
      "Epoch 100/400, Loss: 0.7267\n",
      "Epoch 110/400, Loss: 0.7198\n",
      "Epoch 120/400, Loss: 0.7151\n",
      "Epoch 130/400, Loss: 0.7113\n",
      "Epoch 140/400, Loss: 0.7095\n",
      "Epoch 150/400, Loss: 0.7087\n",
      "Epoch 160/400, Loss: 0.7083\n",
      "Epoch 170/400, Loss: 0.7081\n",
      "Epoch 180/400, Loss: 0.7079\n",
      "Epoch 190/400, Loss: 0.7079\n",
      "Epoch 200/400, Loss: 0.7078\n",
      "Epoch 210/400, Loss: 0.7078\n",
      "Epoch 220/400, Loss: 0.7078\n",
      "Epoch 230/400, Loss: 0.7078\n",
      "Epoch 240/400, Loss: 0.7078\n",
      "Epoch 250/400, Loss: 0.7078\n",
      "Epoch 260/400, Loss: 0.7078\n",
      "Epoch 270/400, Loss: 0.7078\n",
      "Epoch 280/400, Loss: 0.7078\n",
      "Epoch 290/400, Loss: 0.7078\n",
      "Epoch 300/400, Loss: 0.7078\n",
      "Epoch 310/400, Loss: 0.7078\n",
      "Epoch 320/400, Loss: 0.7078\n",
      "Epoch 330/400, Loss: 0.7078\n",
      "Epoch 340/400, Loss: 0.7078\n",
      "Epoch 350/400, Loss: 0.7078\n",
      "Epoch 360/400, Loss: 0.7078\n",
      "Epoch 370/400, Loss: 0.7078\n",
      "Epoch 380/400, Loss: 0.7078\n",
      "Epoch 390/400, Loss: 0.7078\n",
      "Epoch 400/400, Loss: 0.7078\n",
      "Training model for case 3...\n",
      "Epoch 10/400, Loss: 20.6717\n",
      "Epoch 20/400, Loss: 15.4864\n",
      "Epoch 30/400, Loss: 10.6287\n",
      "Epoch 40/400, Loss: 5.7487\n",
      "Epoch 50/400, Loss: 2.0443\n",
      "Epoch 60/400, Loss: 0.7677\n",
      "Epoch 70/400, Loss: 0.8317\n",
      "Epoch 80/400, Loss: 0.7476\n",
      "Epoch 90/400, Loss: 0.7319\n",
      "Epoch 100/400, Loss: 0.7263\n",
      "Epoch 110/400, Loss: 0.7215\n",
      "Epoch 120/400, Loss: 0.7183\n",
      "Epoch 130/400, Loss: 0.7168\n",
      "Epoch 140/400, Loss: 0.7161\n",
      "Epoch 150/400, Loss: 0.7157\n",
      "Epoch 160/400, Loss: 0.7155\n",
      "Epoch 170/400, Loss: 0.7154\n",
      "Epoch 180/400, Loss: 0.7154\n",
      "Epoch 190/400, Loss: 0.7154\n",
      "Epoch 200/400, Loss: 0.7154\n",
      "Epoch 210/400, Loss: 0.7154\n",
      "Epoch 220/400, Loss: 0.7154\n",
      "Epoch 230/400, Loss: 0.7154\n",
      "Epoch 240/400, Loss: 0.7154\n",
      "Epoch 250/400, Loss: 0.7153\n",
      "Epoch 260/400, Loss: 0.7153\n",
      "Epoch 270/400, Loss: 0.7153\n",
      "Epoch 280/400, Loss: 0.7153\n",
      "Epoch 290/400, Loss: 0.7153\n",
      "Epoch 300/400, Loss: 0.7153\n",
      "Epoch 310/400, Loss: 0.7153\n",
      "Epoch 320/400, Loss: 0.7153\n",
      "Epoch 330/400, Loss: 0.7153\n",
      "Epoch 340/400, Loss: 0.7153\n",
      "Epoch 350/400, Loss: 0.7153\n",
      "Epoch 360/400, Loss: 0.7153\n",
      "Epoch 370/400, Loss: 0.7153\n",
      "Epoch 380/400, Loss: 0.7153\n",
      "Epoch 390/400, Loss: 0.7153\n",
      "Epoch 400/400, Loss: 0.7153\n",
      "Training model for case 4...\n",
      "Epoch 10/400, Loss: 23.9563\n",
      "Epoch 20/400, Loss: 18.5758\n",
      "Epoch 30/400, Loss: 13.2615\n",
      "Epoch 40/400, Loss: 7.9333\n",
      "Epoch 50/400, Loss: 3.2400\n",
      "Epoch 60/400, Loss: 0.9885\n",
      "Epoch 70/400, Loss: 0.7702\n",
      "Epoch 80/400, Loss: 0.8039\n",
      "Epoch 90/400, Loss: 0.7147\n",
      "Epoch 100/400, Loss: 0.7135\n",
      "Epoch 110/400, Loss: 0.7050\n",
      "Epoch 120/400, Loss: 0.7032\n",
      "Epoch 130/400, Loss: 0.7022\n",
      "Epoch 140/400, Loss: 0.7018\n",
      "Epoch 150/400, Loss: 0.7016\n",
      "Epoch 160/400, Loss: 0.7015\n",
      "Epoch 170/400, Loss: 0.7015\n",
      "Epoch 180/400, Loss: 0.7015\n",
      "Epoch 190/400, Loss: 0.7015\n",
      "Epoch 200/400, Loss: 0.7015\n",
      "Epoch 210/400, Loss: 0.7015\n",
      "Epoch 220/400, Loss: 0.7015\n",
      "Epoch 230/400, Loss: 0.7015\n",
      "Epoch 240/400, Loss: 0.7015\n",
      "Epoch 250/400, Loss: 0.7015\n",
      "Epoch 260/400, Loss: 0.7015\n",
      "Epoch 270/400, Loss: 0.7015\n",
      "Epoch 280/400, Loss: 0.7015\n",
      "Epoch 290/400, Loss: 0.7015\n",
      "Epoch 300/400, Loss: 0.7015\n",
      "Epoch 310/400, Loss: 0.7015\n",
      "Epoch 320/400, Loss: 0.7015\n",
      "Epoch 330/400, Loss: 0.7015\n",
      "Epoch 340/400, Loss: 0.7015\n",
      "Epoch 350/400, Loss: 0.7015\n",
      "Epoch 360/400, Loss: 0.7015\n",
      "Epoch 370/400, Loss: 0.7015\n",
      "Epoch 380/400, Loss: 0.7015\n",
      "Epoch 390/400, Loss: 0.7015\n",
      "Epoch 400/400, Loss: 0.7015\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, smiles_dim, amino_dim, hidden_dim):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "\n",
    "        # SMILES branch\n",
    "        self.fc_smiles = nn.Sequential(\n",
    "            nn.Linear(smiles_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Amino acid branch\n",
    "        self.fc_amino = nn.Sequential(\n",
    "            nn.Linear(amino_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Combined branch\n",
    "        self.fc_combined = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 4, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, smiles, amino):\n",
    "        # Pass through SMILES branch\n",
    "        x_smiles = self.fc_smiles(smiles)\n",
    "\n",
    "        # Pass through Amino acid branch\n",
    "        x_amino = self.fc_amino(amino)\n",
    "\n",
    "        # Combine and pass through combined branch\n",
    "        x_combined = torch.cat((x_smiles, x_amino), dim=1)\n",
    "        output = self.fc_combined(x_combined)\n",
    "        return output\n",
    "\n",
    "# Input dimensions\n",
    "X_amino_1, X_smiles_1 = process_data(train_df_1)\n",
    "X_amino_2, X_smiles_2 = process_data(train_df_2)\n",
    "X_amino_3, X_smiles_3 = process_data(train_df_3)\n",
    "X_amino_4, X_smiles_4 = process_data(train_df_4)\n",
    "\n",
    "# Hidden dimension can be adjusted as needed\n",
    "hidden_dim = 128\n",
    "\n",
    "# Define models for each split\n",
    "model_case1 = FeedForwardNN(X_smiles_1.shape[1], X_amino_1.shape[1], hidden_dim).to(device)\n",
    "model_case2 = FeedForwardNN(X_smiles_2.shape[1], X_amino_2.shape[1], hidden_dim).to(device)\n",
    "model_case3 = FeedForwardNN(X_smiles_3.shape[1], X_amino_3.shape[1], hidden_dim).to(device)\n",
    "model_case4 = FeedForwardNN(X_smiles_4.shape[1], X_amino_4.shape[1], hidden_dim).to(device)\n",
    "\n",
    "# Loss function and optimizer for each model\n",
    "criterion_case1 = nn.MSELoss()\n",
    "criterion_case2 = nn.MSELoss()\n",
    "criterion_case3 = nn.MSELoss()\n",
    "criterion_case4 = nn.MSELoss()\n",
    "optimizer_case1 = optim.Adam(model_case1.parameters(), lr=0.001)\n",
    "optimizer_case2 = optim.Adam(model_case2.parameters(), lr=0.001)\n",
    "optimizer_case3 = optim.Adam(model_case3.parameters(), lr=0.001)\n",
    "optimizer_case4 = optim.Adam(model_case4.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_df, test_df, train_labels, epochs=400):\n",
    "    model.train()\n",
    "    \n",
    "    # Convert training data to tensors\n",
    "    X_smiles_train = torch.tensor(train_df['encoded_SMILES'].tolist(), dtype=torch.float32).to(device)\n",
    "    X_amino_train = torch.tensor(train_df['padded_sequence'].tolist(), dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(train_labels.tolist(), dtype=torch.float32).to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(X_smiles_train, X_amino_train).squeeze()\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, y_train)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "# Train each model with respective splits\n",
    "print(\"Training model for case 1...\")\n",
    "train_model(model_case1, criterion_case1, optimizer_case1, train_df_1, test_df_1, train_labels_1)\n",
    "\n",
    "print(\"Training model for case 2...\")\n",
    "train_model(model_case2, criterion_case2, optimizer_case2, train_df_2, test_df_2, train_labels_2)\n",
    "\n",
    "print(\"Training model for case 3...\")\n",
    "train_model(model_case3, criterion_case3, optimizer_case3, train_df_3, test_df_3, train_labels_3)\n",
    "\n",
    "print(\"Training model for case 4...\")\n",
    "train_model(model_case4, criterion_case4, optimizer_case4, train_df_4, test_df_4, train_labels_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97860a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model for Case 1...\n",
      "Mean Squared Error (MSE): 2015.7471\n",
      "R^2: -2625.0547\n",
      "\n",
      "Evaluating Model for Case 2...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "linear(): input and weight.T shapes cannot be multiplied (3060x2027 and 2549x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 83\u001b[0m\n\u001b[1;32m     80\u001b[0m evaluate_model(model_case1, X_smiles_t_1, X_amino_t_1, test_labels_1, device)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating Model for Case 2...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 83\u001b[0m evaluate_model(model_case2, X_smiles_t_2, X_amino_t_2, test_labels_2, device)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating Model for Case 3...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m evaluate_model(model_case3, X_smiles_t_3, X_amino_t_3, test_labels_3, device)\n",
      "Cell \u001b[0;32mIn[12], line 31\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, X_smiles_test, X_amino_test, y_test, device)\u001b[0m\n\u001b[1;32m     28\u001b[0m y_test_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y_test_numpy, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Predictions\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model(X_smiles_test, X_amino_test)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Move predictions to CPU and convert to NumPy (fixed)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m predictions \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# Move to CPU before numpy conversion\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 54\u001b[0m, in \u001b[0;36mFeedForwardNN.forward\u001b[0;34m(self, smiles, amino)\u001b[0m\n\u001b[1;32m     51\u001b[0m x_smiles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_smiles(smiles)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Pass through Amino acid branch\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m x_amino \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_amino(amino)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Combine and pass through combined branch\u001b[39;00m\n\u001b[1;32m     57\u001b[0m x_combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x_smiles, x_amino), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: linear(): input and weight.T shapes cannot be multiplied (3060x2027 and 2549x128)"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Concordance Index (CI) implementation\n",
    "def concordance_index(y_true, y_pred):\n",
    "    n = 0\n",
    "    h_sum = 0\n",
    "    for i in range(len(y_true)):\n",
    "        for j in range(i + 1, len(y_true)):\n",
    "            if y_true[i] != y_true[j]:\n",
    "                n += 1\n",
    "                if (y_pred[i] > y_pred[j] and y_true[i] > y_true[j]) or \\\n",
    "                   (y_pred[i] < y_pred[j] and y_true[i] < y_true[j]):\n",
    "                    h_sum += 1\n",
    "                elif y_pred[i] == y_pred[j]:\n",
    "                    h_sum += 0.5\n",
    "    return h_sum / n if n > 0 else 0.0\n",
    "\n",
    "# Updated Evaluation Function\n",
    "# Updated Evaluation Function\n",
    "def evaluate_model(model, X_smiles_test, X_amino_test, y_test, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Convert y_test (Pandas Series) to NumPy array before passing to torch.tensor\n",
    "        y_test_numpy = y_test.to_numpy()  # Convert to NumPy array\n",
    "        y_test_tensor = torch.tensor(y_test_numpy, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Predictions\n",
    "        predictions = model(X_smiles_test, X_amino_test)\n",
    "        \n",
    "        # Move predictions to CPU and convert to NumPy (fixed)\n",
    "        predictions = predictions.cpu().detach().numpy()  # Move to CPU before numpy conversion\n",
    "        \n",
    "        # Convert y_test_tensor to NumPy for comparison\n",
    "        y_test_np = y_test_tensor.cpu().detach().numpy()  # Ensure y_test_tensor is on CPU\n",
    "        \n",
    "        # Metrics\n",
    "#         ci = concordance_index(y_test_np, predictions)\n",
    "        mse = mean_squared_error(y_test_np, predictions)\n",
    "        r2 = r2_score(y_test_np, predictions)\n",
    "#         pearson_corr, _ = pearsonr(y_test_np, predictions)\n",
    "        \n",
    "        # Print Metrics\n",
    "#         print(f'Concordance Index (CI): {ci:.4f}')\n",
    "        print(f'Mean Squared Error (MSE): {mse:.4f}')\n",
    "        print(f'R^2: {r2:.4f}')\n",
    "#         print(f'Pearson Correlation Coefficient (R): {pearson_corr:.4f}')\n",
    "\n",
    "        \n",
    "X_amino_t_1, X_smiles_t_1 = process_data(test_df_1)\n",
    "X_amino_t_2, X_smiles_t_2 = process_data(test_df_2)\n",
    "X_amino_t_3, X_smiles_t_3 = process_data(test_df_3)\n",
    "X_amino_t_4, X_smiles_t_4 = process_data(test_df_4)\n",
    "\n",
    "X_amino_t_1 = X_amino_t_1.to(device)\n",
    "X_amino_t_2 = X_amino_t_2.to(device)\n",
    "X_amino_t_3 = X_amino_t_3.to(device)\n",
    "X_amino_t_4 = X_amino_t_4.to(device)\n",
    "\n",
    "X_smiles_t_1 = X_smiles_t_1.to(device)\n",
    "X_smiles_t_2 = X_smiles_t_2.to(device)\n",
    "X_smiles_t_3 = X_smiles_t_3.to(device)\n",
    "X_smiles_t_4 = X_smiles_t_4.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# X_smiles_t_1 = torch.tensor(test_df_1['encoded_SMILES'].tolist(), dtype=torch.float32).to(device)\n",
    "# X_amino_t_1 = torch.tensor(test_df_1['padded_sequence'].tolist(), dtype=torch.float32).to(device)\n",
    "# X_smiles_t_2 = torch.tensor(test_df_2['encoded_SMILES'].tolist(), dtype=torch.float32).to(device)\n",
    "# X_amino_t_2 = torch.tensor(test_df_2['padded_sequence'].tolist(), dtype=torch.float32).to(device)\n",
    "# X_smiles_t_3 = torch.tensor(test_df_3['encoded_SMILES'].tolist(), dtype=torch.float32).to(device)\n",
    "# X_amino_t_3 = torch.tensor(test_df_3['padded_sequence'].tolist(), dtype=torch.float32).to(device)\n",
    "# X_smiles_t_4 = torch.tensor(test_df_4['encoded_SMILES'].tolist(), dtype=torch.float32).to(device)\n",
    "# X_amino_t_4 = torch.tensor(test_df_4['padded_sequence'].tolist(), dtype=torch.float32).to(device)\n",
    "\n",
    "# Evaluate Models\n",
    "print(\"Evaluating Model for Case 1...\")\n",
    "evaluate_model(model_case1, X_smiles_t_1, X_amino_t_1, test_labels_1, device)\n",
    "\n",
    "print(\"\\nEvaluating Model for Case 2...\")\n",
    "evaluate_model(model_case2, X_smiles_t_2, X_amino_t_2, test_labels_2, device)\n",
    "\n",
    "print(\"\\nEvaluating Model for Case 3...\")\n",
    "evaluate_model(model_case3, X_smiles_t_3, X_amino_t_3, test_labels_3, device)\n",
    "\n",
    "print(\"\\nEvaluating Model for Case 4...\")\n",
    "evaluate_model(model_case4, X_smiles_t_4, X_amino_t_4, test_labels_4, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
